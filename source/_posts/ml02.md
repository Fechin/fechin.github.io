---
title: 机器学习实战（2）kNN - k 近邻算法
date: 2018-06-03 12:36:23
tags:
cover: http://odwjyz4z6.bkt.clouddn.com/ml02/ml02cover.jpg
mp3: http://oybheyjxt.bkt.clouddn.com/%E6%80%80%E5%BF%B5%E9%9D%92%E6%98%A5.mp3
---

#### 什么是 k 近邻算法
k 近邻（k-Nearest Neighbor，kNN）是机器学习中相对简单且容易理解的算法，是分类数据最简单有效的算法，它基于实例学习，我们必须用最真实的样本数据对模型进行训练， 以便在预测时达到更高的准确度。

简单地说，**k 近邻算法采用测量不同特征值之间的距离方法进行分类**。这句话简单也不简单，更通俗易懂的理解：
> 所谓“物以类聚，人以群分”，kNN 认为彼此相隔最近的点为一类。周围的对象是什么类别，我就是什么类别。如果你的朋友都开法拉利、保时捷等跑车，那么在 kNN 眼你，你也是有钱人，也开豪车。

#### 算法实现
本文不打算用代码实现一个 k 近邻算法，感兴趣的可以查阅资料研究其实现，但我们必须知道算法的实现步骤。
> 对未知类型的数据集中的每个点依次执行如下操作：
1. 计算已知类别数据集中的点与当前点之间的距离；
2. 按距离递增次序排序；
3. 选取与当前点距离最小的 k 个点；
4. 确认前 k 个点出现频率最高的类别作为当前点的预测分类；

接下来，我会举个栗子带大家更好的理解 kNN 算法。如图，有三种类别的人，天蓝色的你、黄色的土豪和午夜蓝的月光族，现在想通过你的总收入预测你的类别。

![Alt text](http://odwjyz4z6.bkt.clouddn.com/ml02/kNN-sample.jpg)

我们拿到了七个样本数据，每一个样本带有工龄、年薪、总收入（工龄 x 年薪）三个特征，和对应的类别标签。分布在图中的数据如下：

| 姓名  |  工龄（年） | 年薪（万）| 总收入（万）|  类别  |
| :---- | :----------:| :-------: |:----------: |:-----: |
| 张三  | 2           | 18        | 36          |  月光  |
| 李四  | 3           | 110       | 330         |  土豪  |
| 王五  | 5           | 70        | 350         |  月光  |
| 赵六  | 4           | 30        | 120         |  月光  |
| 孙七  | 7           | 110       | 770         |  土豪  |
| 周八  | 9           | 76        | 684         |  土豪  |
| 吴九  | 10          | 80        | 800         |  土豪  |
| 你    | 5           | 90        | 450         |  ？    |

现在的需求是根据样本数据，以及你的特征预测出你是“土豪”还是“月光”，按照 kNN 算法的实现步骤，计算你的总收入与样本集的距离（欧式距离公式）。升序排序之后结果如下：

| 姓名  | 总收入（万）|距离（万）↑|  类别  |
| :---- |:----------: |:-------: |:-----: |
| 王五  | 350         |100       |  月光  |
| 李四  | 330         |120       |  土豪  |
| 周八  | 684         |234       |  土豪  |
| 孙七  | 770         |320       |  土豪  |
| 赵六  | 120         |330       |  月光  |
| 吴九  | 800         |350       |  土豪  |
| 张三  | 36          |414       |  月光  |

我们得到了预测数据与各个样本间的距离，并按递增排好序。现假设 k=3，那么对应的类别分别是“月光”、“土豪”、“土豪”，按照算法实现的第 4 步，前 3 个点出现频率最高的类别是“土豪”。那么，我们得到了预测结果，你是土豪，开心不开心！

实例中 k 值取多少没有定论，如果实例中 k = 1 或 k = 2，那么结果未必是最准确的，所以对于 k 值的把握建议尽可能的大于等于 3，这是一个经验值。

我们回过头来看数据分布图，以你为中心画圆，其中圆 c1 也就是 k = 1，距你最近的是“月光”，圆 c2 则是当 k = 4 的时候，其中三个“土豪”，那么也能证明你是真土豪！

#### k 近邻算法优缺点
优点：精度高、对异常值不敏感、无数据输入假定。
缺点：kNN 必须保存全部数据集，会占用大量的存储空间，所以**空间负责度高**；kNN 必须对数据集中的每个数据计算距离值，实际使用时非常耗时，所以**计算复杂度高**。

