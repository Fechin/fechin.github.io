
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Pandas 和 PySpark 的 DataFrame 相互转换 - 常也</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="
Pandas DataFrame 是一个二维数组，跟数据库中的 Table 或 Excel 很相似，底层使用 Numpy 和 array 存储，Numpy 使用 C 语言编写，运行速度很快。在 P,"> 
    <meta name="author" content="Fechin"> 
    <link rel="alternative" href="atom.xml" title="常也" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Pandas 和 PySpark 的 DataFrame 相互转换 - 常也"/>
    <meta name="twitter:description" content="
Pandas DataFrame 是一个二维数组，跟数据库中的 Table 或 Excel 很相似，底层使用 Numpy 和 array 存储，Numpy 使用 C 语言编写，运行速度很快。在 P,"/>
    
    
    
    
    <meta property="og:site_name" content="常也"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="Pandas 和 PySpark 的 DataFrame 相互转换 - 常也"/>
    <meta property="og:description" content="
Pandas DataFrame 是一个二维数组，跟数据库中的 Table 或 Excel 很相似，底层使用 Numpy 和 array 存储，Numpy 使用 C 语言编写，运行速度很快。在 P,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

    <script>window.searchDbPath = "/search.xml";</script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">常也</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://fech.in"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">Pandas 和 PySpark 的 DataFrame 相互转换</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Pandas 和 PySpark 的 DataFrame 相互转换</h1>
        <div class="stuff">
            <span>六月 26, 2018</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/PySpark/" rel="tag">PySpark</a></li></ul>


        </div>
        <div class="content markdown">
            <p><img src="/static/images/PandasToSparkDataFrame/pyspark-pandas_cover5.jpg" alt="Pandas to PySpark"></p>
<p>Pandas DataFrame 是一个二维数组，跟数据库中的 Table 或 Excel 很相似，底层使用 Numpy 和 array 存储，Numpy 使用 C 语言编写，运行速度很快。在 Python 语言中，它们是必不可少的机器学习库。</p>
<p>因为 Pandas 的高效，实际开发中，我更多的会借助它做抽样数据分析，然而生产环境的数据量巨大，需要集群部署，所以生产环境用 Spark 全量运行。Spark 和 Pandas 都可以集成 SQL 能力，但它们支持的 SQL 规范不一致，为保持操作统一，我们可能会选择其中的一种规范，也就有了 Pandas 和 PySpark 数据结构的转换需求。</p>
<h4 id="PySpark-DataFrame-转-Pandas-DataFrame"><a href="#PySpark-DataFrame-转-Pandas-DataFrame" class="headerlink" title="PySpark DataFrame 转 Pandas DataFrame"></a>PySpark DataFrame 转 Pandas DataFrame</h4><p>获取或创建 SparkSession</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在集群 UI 上展示的应用程序名称</span></span><br><span class="line">appName = <span class="string">&#x27;Spark2Pandas&#x27;</span></span><br><span class="line"><span class="comment"># 集群的 master URL，本地测试可以是“local”</span></span><br><span class="line">master = <span class="string">&#x27;local&#x27;</span></span><br><span class="line">spark = SparkSession.builder.appName(appName).master(master).getOrCreate()</span><br></pre></td></tr></table></figure>


<p>初始化 Spark 上下文</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line">sc = spark.sparkContext</span><br></pre></td></tr></table></figure>

<p>生成测试数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spf = sc.parallelize([(<span class="number">1</span>, <span class="string">&quot;foo&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;bar&quot;</span>)]).toDF([<span class="string">&quot;id&quot;</span>, <span class="string">&quot;value&quot;</span>])</span><br><span class="line">spf.show()</span><br><span class="line"></span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="built_in">id</span>|value|</span><br><span class="line">+---+-----+</span><br><span class="line">|  <span class="number">1</span>|  foo|</span><br><span class="line">|  <span class="number">2</span>|  bar|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>

<p>PySpark 提供了 toPandas 方法，返回一个 Pandas DataFrame 对象，需要注意，不是所有的类型都可以 toPandas，目前尚不支持的数据类型有 ArrayType、 MapType，跟进问题处理进度请参考Jira <a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/SPARK-21187">SPARK-21187</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pdf = spf.toPandas()</span><br><span class="line"><span class="built_in">print</span> pdf.head()</span><br><span class="line"></span><br><span class="line">	<span class="built_in">id</span> value</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   foo</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>   bar</span><br></pre></td></tr></table></figure>

<h5 id="空类型处理"><a href="#空类型处理" class="headerlink" title="空类型处理"></a>空类型处理</h5><p>在 PySpark 转 Pandas 之前，可能需要对数据做一些预处理，如修正数据，数据类型兼容等。下面这个例子是把空的 ArrayType、MapType 转换为 None。读者可举一反三，自行定制。另外，我们还可以通过Spark udf函数达到这个目的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> when, size, col, lit</span><br><span class="line"><span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(spf.schema.fields):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(f.dataType, ArrayType) \</span><br><span class="line">    <span class="keyword">or</span> <span class="built_in">isinstance</span>(f.dataType, MapType):</span><br><span class="line">        spf = spf.withColumn(f.name, when(size(col(f.name)) == <span class="number">0</span> , lit(<span class="literal">None</span>)).otherwise(col(f.name) ) )</span><br></pre></td></tr></table></figure>

<h4 id="Pandas-DataFrame-转-PySpark-DataFrame"><a href="#Pandas-DataFrame-转-PySpark-DataFrame" class="headerlink" title="Pandas DataFrame 转 PySpark DataFrame"></a>Pandas DataFrame 转 PySpark DataFrame</h4><p>PySpark 的 <code>createDataFrame(data, schema=None, samplingRatio=None)</code> 非常强大，它不仅支持 RDD、Python 元组和列表作为输入，还可以是 Pandas DataFrame，其内部会自动进行转换，其原理可以参考下文PySpark实现原理部分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.createDataFrame(pdf)</span><br></pre></td></tr></table></figure>

<h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><p>下面我们来看看 Pandas 和 PySpark 的 DataFrame 转换的性能，笔者使用的个人开发机，配置如下：</p>
<blockquote>
<p>MacBook Pro (Retina, 13-inch, Late 2013)<br>处理器：2.4 GHz Intel Core i5<br>内存：8 GB 1600 MHz DDR3</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_rows = <span class="number">500000</span></span><br><span class="line">pdf = pd.DataFrame(np.random.randn(n_rows, <span class="number">20</span>))</span><br><span class="line">%time spf = sqlContext.createDataFrame(pdf)</span><br><span class="line"></span><br><span class="line">CPU times: user 1<span class="built_in">min</span> 32s, sys: <span class="number">1.08</span> s, total: 1<span class="built_in">min</span> 33s</span><br><span class="line">Wall time: 1<span class="built_in">min</span> 37s</span><br></pre></td></tr></table></figure>
<p>当 Pandas DataFrame 行数为 n_rows ，列固定 20 ，将其转换为 PySpark DataFrame 耗时如下：</p>
<p><img src="/static/images/PandasToSparkDataFrame/PandasToSparkDataFrame_withoutarrow_mini.jpg" alt="PySpark to Pandas"><br>随着行数的增加，越来越耗时，100w 条数据时耗时已经超过 200 秒，笔者也测试了从 PySpark DataFrame 到 Pandas DataFrame 的转换，耗时基本一致，时间都去哪儿了？</p>
<h4 id="PySpark-实现原理"><a href="#PySpark-实现原理" class="headerlink" title="PySpark 实现原理"></a>PySpark 实现原理</h4><p>Spark 是由 JVM 语言实现，并且程序会运行在 JVM 之上，无论是 Python、R 还是其它语言实现的 Spark，都会间接的和 JVM 进行通信，获得处理海量数据的能力。<br><img src="/static/images/PandasToSparkDataFrame/PySpark2_mini.jpg" alt="Pandas to PySpark"></p>
<p>Python 借助 Py4j 实现和 Java 的交互，一个 PySpark 程序启动时，会实例化 Python 版的 SparkContext 对象，这也是一个比较耗时的过程，其内部实现：</p>
<ol>
<li>实例化 Py4j GatewayClient，连接 JVM 中的 Py4j GatewayServer</li>
<li>通过 Py4j Gateway 在 JVM 中实例化 SparkContext 对象</li>
</ol>
<p>以上，大概解释了为什么创建 SparkContext 慢的原因。那么，Pandas DataFrame 转 PySpark DataFrame 慢是为什么呢？原因有三：</p>
<ol>
<li>PySpark 并没有直接使用 Pandas DataFrame 的数据类型，而是试图推断数据。</li>
<li>PySpark 不能使用高效的 NumPy 库，并且会迭代每一条记录，将记录中的值转换为 Python 对象。</li>
<li>通过 Py4j 发送到 JVM 时，必须序列化成 pickle 格式，Spark 会通过单独的线程读取文件并转换成 Scala 类型。</li>
</ol>
<p>数据量大时，这一系列步骤将是非常耗时的过程。</p>
<h4 id="使用-Arrow-提高转换速度"><a href="#使用-Arrow-提高转换速度" class="headerlink" title="使用 Arrow 提高转换速度"></a>使用 Arrow 提高转换速度</h4><p>从 Spark 2.3 开始，集成了 Apache Arrow 支持，Pandas DataFrame 可以高效地转换为 Arrow 数据并直接传输到 JVM 以创建 Spark DataFrame，性能会更好，不在需要类型推断，参考 <a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/SPARK-20791">SPARK-20791</a>。开启 Arrow 后的执行原理：</p>
<ol>
<li>根据默认并行数将 Pandas DataFrame 切片</li>
<li>将每个 Pandas 切片转换为 Arrow RecordBatch</li>
<li>将 schema 从 Arrow 转换为 Spark</li>
<li>将 RecordBatchES 发送给 JVM，作为 JavaRDD[Array[Byte]]</li>
<li>使用 Spark schema 包装 JavaRDD 并创建 DataFrame</li>
</ol>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以在 spark-defaults.conf 全局配置</span></span><br><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;spark.sql.execution.arrow.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">spf = spark.createDataFrame(pdDF)</span><br></pre></td></tr></table></figure>
<p>如图，使用 Arrow 之后，测试 100w 条数据仅仅用了 1.2 秒<br><img src="/static/images/PandasToSparkDataFrame/PandasToSparkDataFrame_witharrow_mini.jpg" alt="PySpark to Pandas"></p>
<p>跟不使用 Arrow 相比，差得不是一点半点啊，下图中的红线是使用 Arrow 的耗时，基本与 X 轴重合了，这么高的反差，笔者感到很惊讶。<br><img src="/static/images/PandasToSparkDataFrame/PandasToSparkDataFrame_twoline_mini.jpg" alt="PySpark to Pandas"></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="/static/mp3/That%20Girl.mp3">
            </audio>
            
        </div>
        
        
    <div id="gitalk-container" class="comment link"
		data-enable="true"
        data-ae="false"
        data-ci="a396afe529734b15300f"
        data-cs="0ae7c1790f9a5cbc2b7339c12a82d58fe58f9567"
        data-r="fechin.github.io"
        data-o="Fechin"
        data-a="Fechin"
        data-d="false"
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>





<!-- Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-69833742-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-69833742-2');
</script>
<!-- End Google Analytics -->


</html>
